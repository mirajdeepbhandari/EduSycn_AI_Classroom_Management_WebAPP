{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGUCBTNh1gul",
        "outputId": "3738e302-7ad7-4878-ad0b-5ca2d1a6206a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community -q\n",
        "!pip install pypdf -q\n",
        "!pip install langchain_google_genai -q\n",
        "! pip install PyMuPDF -q\n",
        "!pip install python-pptx -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate,PromptTemplate"
      ],
      "metadata": {
        "id": "NcwzuGof1xDU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO JOIN ALL THE TEXT FROM ALL THE PAGES OF THE PDF DOCUMENTS\n",
        "\n",
        "def doc_words_join(inp):\n",
        "   loader = PyPDFLoader(inp)\n",
        "   pages = loader.load_and_split()\n",
        "   # print(len(pages)) #no of pages in the documents\n",
        "   whole_doc_text = ''\n",
        "   for i in range(len(pages)):\n",
        "       whole_doc_text+=pages[i].page_content\n",
        "   return whole_doc_text"
      ],
      "metadata": {
        "id": "NUB12HxM1y_d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "allwords=doc_words_join('/content/2206784 Miraj Deep Bhandari.pdf')\n",
        "print(allwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVUCRpU813Yb",
        "outputId": "f0894f92-45ee-4c5c-a036-00eb982e7cec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module Code & Module Title \n",
            "CU6051NI Artificial Intelligence \n",
            "Assessment Weightage & Type \n",
            "25% Individual \n",
            "Semester \n",
            "2024 Autumn \n",
            "Project Name: Nepali Image Captioning \n",
            "Student Name: Miraj Deep Bhandari \n",
            "London Met ID: 22067814 \n",
            "College ID: np01cp4a220197 \n",
            "Group :L3C8 \n",
            "Assignment Due Date: Wednesday, 25 December 2024 \n",
            "Assignment Submission Date: Monday, 23 December 2024 \n",
            "I confirm that I understand my coursework needs to be submitted online via Google Classroom \n",
            "under the relevant module page before the deadline in order for my assignment to be accepted \n",
            "and marked. I am fully aware that late submissions will be treated as non-submission and a mark \n",
            "of zero will be awarded.Table of Contents \n",
            "1) Introduction: .............................................................................................................. 1 \n",
            "1.1 Aims: ..................................................................................................................... 2 \n",
            "1.2 Objectives: ........................................................................................................... 2 \n",
            "1.3 Problem Domain & Use Cases: .......................................................................... 3 \n",
            "2) Background: .............................................................................................................. 5 \n",
            "3) Solution: .................................................................................................................... 9 \n",
            "3.1 Algorithms Used in (ViT & GPT): ................................................................. 12 \n",
            "3.2 Pseudocode: ...................................................................................................... 14 \n",
            "3.3 Diagrammatical representations: ..................................................................... 17 \n",
            "3.3.1 Image Captioning Model Architecture Diagram: ...................................... 17 \n",
            "3.3.2 FlowChart Diagram: .................................................................................... 18 \n",
            "4) Conclusion: ............................................................................................................. 19 \n",
            "4.1 Analysis of the Work Done: .............................................................................. 19 \n",
            "4.2 Addressing Real-World Problems: ................................................................... 20 \n",
            "4.3 Further Work: ..................................................................................................... 20 \n",
            "5) References: ............................................................................................................. 21Table of Figures \n",
            " \n",
            "Figure 1: Image Captioning .......................................................................................... 1 \n",
            "Figure 2: GPT (Decoder) ............................................................................................... 9 \n",
            "Figure 3: Vision Transformer Architecture (Encoder) ................................................ 9 \n",
            "Figure 4: Diagram Representation of Entire Architecture ....................................... 17 \n",
            "Figure 5: Flow Chart Diagram .................................................................................... 181 \n",
            "22067814 Miraj Deep Bhandari \n",
            "1) Introduction: \n",
            " \n",
            "Image Captioning is one of the most interesting areas in the field of AI which lies at the \n",
            "intersection of computer vision and natural language processing (NLP). Image captioning \n",
            "is the task of generating a textual description given an image as input. It melds methods \n",
            "for interpreting visual content with the capacity to generate sensible and context -\n",
            "appropriate language. \n",
            "I am working on Nepali Image Captioning  in Deep Learning  for my project . Image \n",
            "Captioning is a task in the Computer Vision and Natural NLP domain. \n",
            " \n",
            "Computer vision is a branch of AI that allows computers to view and interpret visual data \n",
            "from the surroundings like images and videos. This includes such tasks as: object \n",
            "detection, image classification, segmentation, and creating visual representations, which \n",
            "are all c ritically important for extracting content from the image in a context of image \n",
            "captioning (Computer Vision for Visual Computing: Techniques and Applications, 1998). \n",
            " \n",
            "Figure 1: Image Captioning2 \n",
            "22067814 Miraj Deep Bhandari \n",
            "Natural Language Processing(NLP)  is the Area of AI to make machines able to \n",
            "communicate in Human Language. Natural language processing techniques are applied \n",
            "to create human-readable captions for the images. This type of task requires knowledge \n",
            "of syntax, background, and semantics so as to generate coherent and determination \n",
            "captions (Rao and McMahan, 2019). \n",
            " \n",
            "1.1 Aims: \n",
            "• Create a Deep Learning based system that can Generate the Correct and \n",
            "Corresponding Caption for the Image in the Nepali language. \n",
            "• Use a combination of the computer vision and the natural language processing to \n",
            "properly interpret the visual content and describe it in natural text. \n",
            "• Domain-specific linguistic rules and visual content understanding to improve \n",
            "quality of Nepali image captioning. \n",
            "• Help in the development of AI applications for low resource languages such as \n",
            "Nepali. \n",
            " \n",
            "1.2 Objectives: \n",
            "• Use modern Computer Vision algorithms like Vison Transformers for feature \n",
            "extraction from the images. \n",
            "• Use sequence generation language model such as a transformer to generate \n",
            "Nepali captions from the visual features. \n",
            "• For training and evaluation of the image captioning model, datasets in Nepali need \n",
            "to be preprocessed and annotated. \n",
            "• Choose standard metrics (BLEU, METEOR, CIDEr) to evaluate the generated \n",
            "captions for the model. \n",
            "• Overcome challenges related to grammar, semantics, and contextual \n",
            "appropriateness by Fine -tuning and optimizing the model for Nepali text \n",
            "generation.3 \n",
            "22067814 Miraj Deep Bhandari \n",
            "1.3 Problem Domain & Use Cases: \n",
            " \n",
            "Image captioning is essential as it enables machines to describe images using human \n",
            "comprehendible language. Computer vision techniques help us find where these objects \n",
            "are within an image, but they do not tell us what they are in detail. For instance, we can \n",
            "understand when there is a dog in an image, but generating a complete descriptive output \n",
            "like \"A dog running on the beach\" needs to know actions, context and relations.  \n",
            " \n",
            "Image captioning was invented so that machines can argue with humans in the same \n",
            "space, understand the visual world. It emerged from the need for: \n",
            "• Human-Computer Interaction (HCI): HCI (Human-Computer Interaction): \n",
            "HCI is an approach to computing that styles interactions between humans \n",
            "and machines. \n",
            " \n",
            "• Assistive Technologies: Combining processing visual content and \n",
            "generating accessible descriptions to benefit users with visual impairments. \n",
            " \n",
            " \n",
            "• Search and Retrieval:   More searchable image contents through \n",
            "comprehensive captions. \n",
            " \n",
            "• Automation and Efficiency:  Automating image organization and product \n",
            "tagging in ecommerce.4 \n",
            "22067814 Miraj Deep Bhandari \n",
            "Image captioning is useful in several areas: \n",
            "1. Social media: Make automatic image descriptions to improve engagement and \n",
            "accessibility. \n",
            "2. E-commerce: Generating automatic descriptions of products from store images. \n",
            "3. Health: Giving an explanation of medical scans that doctors can better \n",
            "understand. \n",
            "4. Autonomous Vehicles:  Assisting self -driving cars in describing what’s around \n",
            "them. \n",
            "5. Robotics involves providing robots with the ability to understand their \n",
            "surroundings and make better decisions. \n",
            "6. surveillance : involves the inspection of video streams for the purpose of \n",
            "identifying trends or events. \n",
            "7. Education: Supporting the creation of captions for educational content. \n",
            "8. Media and Art: Automating the selection and composition of content.5 \n",
            "22067814 Miraj Deep Bhandari \n",
            "2) Background: \n",
            " \n",
            "In the early years hand -crafted features and rule -based systems dominated the field of \n",
            "image captioning . It was more about retrieving certain visual features from the images \n",
            "and writing captions using predefined templates or simple probabilistic models. \n",
            " \n",
            "1. Rule-Based Systems (Pre 2000s): \n",
            "Earlier approaches relied on handcrafted rules to characterize an image. An \n",
            "image, for example, could have been used to detect objects and attributes \n",
            "of an object (color, shape, or position) and then captions could be generated \n",
            "based on a combination of template and attributes. \n",
            "Example: \n",
            "Detected objects: “Car,” “Red,” “Road.” \n",
            "Caption produced: “A red car on the road.” \n",
            " \n",
            "2. Visual Features and Bag of Words (2000s): \n",
            "These approaches used manually engineered visual features like SIFT \n",
            "(Scale-Invariant Feature Transform)  or HOG (Histogram of Oriented \n",
            "Gradients) to detect objects or detections in the image. \n",
            "These were then compared to some fixed collection of labels/captions with \n",
            "simple statistical models based on K -Nearest Neighbors or Bayesian \n",
            "networks. \n",
            "Limitations: \n",
            "Must be approved upon design features requiring some domain \n",
            "expertise \n",
            "Not generalizable at all and unable to do anything more than the \n",
            "base rate6 \n",
            "22067814 Miraj Deep Bhandari \n",
            "3. Early 2010s: Retrieval-Based Captioning (2000s – Early 2010s): \n",
            "Such systems relied on having a well labelled database of images and \n",
            "rather than generating captions from scratch, they would find the closest \n",
            "matching image in the database and use it caption. \n",
            "One example involved matching images using global image descriptors \n",
            "(e.g., GIST) based on their overall similarity. \n",
            "Limitation:  \n",
            "An inability to generate new captions and generalize with unseen \n",
            "images. \n",
            " \n",
            "The above methods are the traditional ways of generating captions from images. Modern \n",
            "approaches take a different route in this process with: \n",
            "1. Architecture with Encoder and Decoder:  This consists of CNNs (like \n",
            "ResNet) for feature extraction of images and using RNNs (RNNs like \n",
            "LSTMs) to generate captions sequentially. \n",
            " \n",
            "2. Attention Mechanisms: Works by dynamically choosing different relevant \n",
            "parts of the image for each word, instead of calculating one single image \n",
            "per caption. Thus enabling stricter accuracy and detail. \n",
            " \n",
            " \n",
            "3. Transformer: Uses transformers (e.g. Vision Transformers, GPT) for image \n",
            "analysis and caption generation, greater scalability, more fluent, capacity \n",
            "for complex scenes.7 \n",
            "22067814 Miraj Deep Bhandari \n",
            "For developing the image captioning model, I have gone through the following \n",
            "literature reviews: \n",
            " \n",
            " \n",
            " \n",
            "Paper Title Features Implemented Algorithms Used Link \n",
            "Show and Tell: \n",
            "A Neural Image \n",
            "Caption \n",
            "Generator \n",
            "• Uses CNNs to extract features \n",
            "from the image and RNNs to \n",
            "learn to generate a caption \n",
            "describing the image. It uses the \n",
            "encoder-decoder structure with \n",
            "a pre -trained Inception model \n",
            "encoder and a LSTM decoder. \n",
            "• Convolutional Neural \n",
            "Networks (CNNs) with some \n",
            "famous models like Inception \n",
            "For Feature Extraction.  \n",
            "• Long Short Term Memory \n",
            "(LSTM) for sequential \n",
            "language modeling. \n",
            "https://arxiv.org/ab\n",
            "s/1411.4555 \n",
            "Show, Attend, \n",
            "and Tell: Neural \n",
            "Image Caption \n",
            "Generation with \n",
            "Visual Attention \n",
            "• Adds an attention mechanism \n",
            "which enables the model to pay \n",
            "attention to specific areas of the \n",
            "image when generating each \n",
            "word. This enhances the quality \n",
            "and specificity of the captions \n",
            "generated. \n",
            "• CNN (ResNet) to extract \n",
            "features. \n",
            "• Whereas a soft attention \n",
            "mechanism focusing on \n",
            "image regions but also \n",
            "introducing a distracting \n",
            "amount of noise.  \n",
            "• LSTM for caption \n",
            "generation. \n",
            "https://arxiv.org/ab\n",
            "s/1502.03044 \n",
            "Image \n",
            "Captioning with \n",
            "Semantic \n",
            "Attention \n",
            "• Semantic attention which fuses \n",
            "global semantic (e.g., latent \n",
            "structured attributes) and spatial \n",
            "visual features. This \n",
            "enhancement helps in the \n",
            "generation of captions which are \n",
            "semantically relevant. \n",
            "• CNN for extracting spatial \n",
            "features. \n",
            "• Fuse Visual and Semantic \n",
            "features via Semantic \n",
            "Attention Mechanism.  \n",
            "• Long short term memory \n",
            "(for generating words one by \n",
            "one). \n",
            "https://arxiv.org/ab\n",
            "s/1603.039258 \n",
            "22067814 Miraj Deep Bhandari \n",
            "Image \n",
            "Captioning with \n",
            "CNN and RNN \n",
            "• It directly investigates the \n",
            "integration of CNN for extracting \n",
            "visual features and RNN for \n",
            "forming the actual sentences. It \n",
            "shows how CNN-based feature \n",
            "vectors can lead the RNN in \n",
            "caption generation. \n",
            "• Extracting image features \n",
            "using a CNN (VGG16 / \n",
            "ResNet). \n",
            "• Sequentially using GRU \n",
            "(Gated Recurrent Unit) or \n",
            "LSTM to create the caption. \n",
            "https://arxiv.org/ab\n",
            "s/1604.03944 \n",
            "Image \n",
            "Captioning \n",
            "Using Vision \n",
            "Transformers \n",
            "(ViT) \n",
            "• Suggests using Vision \n",
            "Transformers (ViT) instead of \n",
            "CNNs. Transformers can model \n",
            "the global dependencies of \n",
            "images better than CNNs by \n",
            "getting a larger context. \n",
            "• Using Vision Transformer \n",
            "(ViT) as feature extractor.  \n",
            "• Transformer -based \n",
            "Encoder-Decoder with \n",
            "Transformer for caption \n",
            "generation. \n",
            "• The Multi-Head Attention is \n",
            "used to learn dependencies \n",
            "not only between the image \n",
            "regions but also between the \n",
            "image regions and captions \n",
            "too. \n",
            "https://arxiv.org/ab\n",
            "s/2105.01928 \n",
            "Attention Is All \n",
            "You Need \n",
            "• Self attention: it captures \n",
            "semantic relationships between \n",
            "all elements in a sequence.  \n",
            "• Multi -Head Attention: It allows \n",
            "looking at different places in the \n",
            "input at the same time.  \n",
            "• Positional Encoding: Helps in \n",
            "maintaining the order of the \n",
            "sequence. \n",
            "• Encoder-Decoder Architecture: \n",
            "Stacking of both an encoder and \n",
            "a decoder, Encoder -decoder \n",
            "Architecture. \n",
            "• Scaled Dot -Product \n",
            "Attention. \n",
            "• Layer Normalization, \n",
            "Feedforward Networks. \n",
            "https://arxiv.org/ab\n",
            "s/1706.037629 \n",
            "22067814 Miraj Deep Bhandari \n",
            "3) Solution: \n",
            " \n",
            "Image captioning can be  done using different architectures and techniques. But the latest \n",
            "one is using the transformer architecture. The transformer architecture consists of the \n",
            "encoder and decoder parts. \n",
            "The encoder part acts as a vision transformer, and the decoder part acts as the GPT-\n",
            "2 architecture , so overall we can call it (Vision Transformer + GPT -2) image \n",
            "captioning. The encoder part takes the images, and it captures the features of the image, \n",
            "and the decoder part, GPT, takes the text with the vision transformer out of the image \n",
            "features and generates the captions of the image. \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "                                \n",
            "                                                \n",
            "Figure 3: Vision Transformer Architecture (Encoder) \n",
            "Figure 2: GPT (Decoder)10 \n",
            "22067814 Miraj Deep Bhandari \n",
            "Image captioning is used for e-commerce, social media like Facebook, scientific research, \n",
            "self-moving vehicles, OCR technologies, medical fields, and many more. The traditional \n",
            "rule and the probabilistic approach don’t help to generalize the model. The tra ditional \n",
            "models are unable to generate the captions for any language; the entire process also \n",
            "involves a lot of time for making the rules and framework. The images are also not \n",
            "processed properly since they consist of heavy matrices, and the bag of words d oesn't \n",
            "capture the semantic meaning of words, and the output may always vary for the same \n",
            "image, so the transformer-based models come into play. \n",
            " \n",
            "The reasons behind the choice of (Vision Transformer + GPT) for image \n",
            "captioning are as follows: \n",
            " \n",
            "1. Vision Transformer (ViT) for Visual Understanding: \n",
            " \n",
            "Self-Attention Mechanism :  ViT uses self-attention mechanism as their \n",
            "basic process to the image patches. It can capture global dependencies and \n",
            "relationships in the image which act somewhat differently to that of \n",
            "traditional convolutional neural networks (CNNs) which work on local \n",
            "features. \n",
            " \n",
            "Patch-wise Input: ViT divides the images into fixed-size patches and uses \n",
            "them as tokens (like words in text), which makes it easy to match with the \n",
            "token-level processing of GPT-2, and easy for integration. \n",
            " \n",
            "Scalability : ViT can easily scale to large datasets and; If we fine -tune on \n",
            "the full ImageNet dataset, ViT takes advantage of pretraining over massive \n",
            "image datasets, yielding strong visual representations for downstream \n",
            "tasks as image captioning and others.11 \n",
            "22067814 Miraj Deep Bhandari \n",
            "2. GPT for Text Generation: \n",
            " \n",
            "Autoregressive modeling:  As the core functionality behind GPT, \n",
            "autoregressive modeling generates a sequence of words that make the \n",
            "most sense given the previous ones, allowing it to serve as a great caption \n",
            "generator \n",
            " \n",
            "Pretrained Language Model: Using a pretrained GPT can help generate \n",
            "understandable captions as it also has a lot of linguistic knowledge and \n",
            "fluency in the language. \n",
            " \n",
            "Fine-Tuning Capability: GPT can be fine -tuned on paired image -caption \n",
            "datasets which enables sophisticated alignment of visual features with the \n",
            "text description. \n",
            " \n",
            " \n",
            "3. State-of-the-Art Results: \n",
            " \n",
            "Performance: Combined ViT  and GPT -2 often perform better than \n",
            "traditional models since they exploit their strengths as complementary \n",
            "components and achieve state of the art results on image captioning \n",
            "benchmarks. \n",
            " \n",
            "Generalization: This architecture can generalize on a wide variety of \n",
            "datasets to create captions across various types of images and events.12 \n",
            "22067814 Miraj Deep Bhandari \n",
            "3.1 Algorithms Used in (ViT & GPT): \n",
            " \n",
            "The ViT & GPT models consist of various algorithms for image captioning, which are \n",
            "listed below: \n",
            " \n",
            "1. Positional Encoding: \n",
            "Positional Encoding injects the positional information into the transformers \n",
            "that handle inputs in a paralleled manner. It does so by simply adding a \n",
            "positional embedding to learnt word vectors that consists of sin and cos of \n",
            "different frequencies, allowing the models to understand the order of tokens.  \n",
            " \n",
            "2. Self-Attention: \n",
            "Self-Attention enables every token (or image patch) to attend to other \n",
            "tokens in the sequence to build its representation. It is used to capture the \n",
            "semantic meaning between words in sentences. For every token, we \n",
            "convert them to queries, keys, and values vectors. The dot product of the \n",
            "query with all the keys gives the attention scores, which are softmaxed to \n",
            "get weights for the values. It allows the model to relate to each token. \n",
            " \n",
            "3. Multi-Head Attention: \n",
            "Multi-Head Attention divides the query, key, and value vectors into separate \n",
            "subspaces for independent processing via different attention heads. The \n",
            "concatenated outputs are reshaped to form a single vector which allows the \n",
            "model to attend to different connections in the input.13 \n",
            "22067814 Miraj Deep Bhandari \n",
            "4. Masked Attention: \n",
            "Masked Attention is a specific type of self -attention which masks some \n",
            "positions so that the model cannot attend to future tokens in the sequence. \n",
            "This is very useful for autoregressive tasks such as text generation. \n",
            " \n",
            "5. Normalization: \n",
            "We use Layer Normalization to normalize the inputs of a layer by \n",
            "normalizing within the activations of the feature, which has been shown to \n",
            "increase training stability and convergence. \n",
            " \n",
            "6. Cross-Attention: \n",
            "Cross-Attention is like self-attention but works over different sequences (i.e. \n",
            "when a decoder attends to the outputs of an encoder in seq-to-seq). \n",
            " \n",
            "7. Fully Connected Network: \n",
            "A Fully Connected Network (FCN) is also known as a Dense Network, In \n",
            "this architecture, all the neurons from one layer are connected to all the \n",
            "neurons from the next layer. In this structure, all the nodes of one layer \n",
            "receive input from every node of the previous layer, which makes this a \n",
            "densely interconnected structure. \n",
            " \n",
            "8. Linear Layer & Softmax: \n",
            "Linear Layer & Softmax A linear layer projects the final embeddings to logits, \n",
            "and the softmax layer passes these logits to probabilities used in \n",
            "classification or for the generation of outputs.14 \n",
            "22067814 Miraj Deep Bhandari \n",
            "   3.2 Pseudocode: \n",
            " \n",
            "CREATE a class ImageCaptioningSystem \n",
            "DO \n",
            "    DECLARE an instance variable imageEncoder as Vision Transformer \n",
            "    DECLARE an instance variable textDecoder as GPT \n",
            "    DECLARE an instance variable tokenizer for tokenization and detokenization \n",
            " \n",
            "    CREATE a constructor ImageCaptioningSystem \n",
            "    DO \n",
            "        INITIALIZE imageEncoder with a pretrained Vision Transformer model \n",
            "        INITIALIZE textDecoder with a pretrained GPT model \n",
            "        INITIALIZE tokenizer with the tokenizer compatible with GPT \n",
            "    END DO \n",
            " \n",
            "    CREATE a function preprocessImage \n",
            "    DO \n",
            "        DECLARE a parameter image \n",
            "        RESIZE the image to fixed dimensions (e.g., 224x224) \n",
            "        CONVERT the image to a tensor \n",
            "        NORMALIZE the image tensor to match Vision Transformer input requirements \n",
            "        RETURN the preprocessed image \n",
            "    END DO \n",
            " \n",
            "    CREATE a function generateCaption \n",
            "    DO \n",
            "        DECLARE a parameter image15 \n",
            "22067814 Miraj Deep Bhandari \n",
            "        CALL preprocessImage with image and store the result in preprocessedImage \n",
            "        PASS preprocessedImage through imageEncoder to get imageFeatures \n",
            "        INITIALIZE a sequence with a special token <BOS> (beginning of sequence) \n",
            " \n",
            "        WHILE the sequence does not contain <EOS> and the length is less than a maximum \n",
            "        DO \n",
            "            PASS the sequence and imageFeatures to the textDecoder to generate the next token \n",
            "            APPEND the generated token to the sequence \n",
            "        END WHILE \n",
            " \n",
            "        CONVERT the token sequence to text using tokenizer \n",
            "        RETURN the generated caption as a string \n",
            "    END DO \n",
            " \n",
            "    CREATE a function trainModel \n",
            "    DO \n",
            "        LOAD a dataset of image-caption pairs \n",
            "        DECLARE parameters for training such as batch size, learning rate, and epochs \n",
            "        FOR each epoch in total epochs \n",
            "        DO \n",
            "            FOR each batch of image-caption pairs \n",
            "            DO \n",
            "                PREPROCESS images in the batch \n",
            "                TOKENIZE captions in the batch \n",
            "                PASS the images through imageEncoder to get imageFeatures \n",
            "                PASS imageFeatures and tokenized captions through textDecoder for training \n",
            "                CALCULATE the loss between predicted and actual captions16 \n",
            "22067814 Miraj Deep Bhandari \n",
            "                UPDATE model parameters using backpropagation \n",
            "            END FOR \n",
            "        END FOR \n",
            "        SAVE the trained model \n",
            "    END DO \n",
            " \n",
            "    CREATE a function testModel \n",
            "    DO \n",
            "        DECLARE a parameter testDataset \n",
            "        FOR each test image in testDataset \n",
            "        DO \n",
            "            CALL generateCaption with the test image \n",
            "            PRINT the test image and generated caption \n",
            "        END FOR \n",
            "    END DO \n",
            "END DO \n",
            " \n",
            "CREATE a main function \n",
            "DO \n",
            "    DECLARE an input image as inputImage \n",
            "    CREATE an instance captioningSystem of ImageCaptioningSystem \n",
            "    CALL captioningSystem.trainModel to train the model \n",
            "    CALL captioningSystem.generateCaption with inputImage to generate a caption \n",
            "    DISPLAY the caption \n",
            "    CALL captioningSystem.testModel with a test dataset to evaluate the model \n",
            "END DO17 \n",
            "22067814 Miraj Deep Bhandari \n",
            "3.3 Diagrammatical representations: \n",
            " \n",
            " \n",
            "3.3.1 Image Captioning Model Architecture Diagram: \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Figure 4: Diagram Representation of Entire Architecture18 \n",
            "22067814 Miraj Deep Bhandari \n",
            "      3.3.2 FlowChart Diagram: \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Figure 5: Flow Chart Diagram19 \n",
            "22067814 Miraj Deep Bhandari \n",
            "4) Conclusion: \n",
            " \n",
            " \n",
            "This project on Image captioning with Vision Transformers using GPT models is \n",
            "an effective demonstration of how we can combine Machine learning and Deep \n",
            "Learning technologies to create a solution for a real -world problem. By \n",
            "understanding both visual and textual information, the solution produces accurate \n",
            "and contextually relevant captions for a variety of images.  \n",
            " \n",
            "4.1 Analysis of the Work Done: \n",
            " \n",
            "The implementation build  the image captioning model all from scratch, showing \n",
            "good practice in building and training models using deep learning. The Vision \n",
            "Transformers were used for obtaining relevant visual features whereas the text \n",
            "generation part made use of GPT architecture for generating a semantically \n",
            "relevant human-readable description. \n",
            " \n",
            "To enhance the tokenization process and the generation of captions specifically \n",
            "for the Nepali language, a pre -trained BERT Nepali tokenizer was incorporated. \n",
            "This emphasizes the point that the highly personalized solution was crafted without \n",
            "iterative use  of tuned -focused, pre -trained models, and that it reflects new, \n",
            "improved capabilities in model design and performance over the rule -based or \n",
            "probabilistic approaches of the past.20 \n",
            "22067814 Miraj Deep Bhandari \n",
            "4.2 Addressing Real-World Problems: \n",
            " \n",
            "            This solution supports many applications including: \n",
            "Visual Impairments: Helping visually impaired users of all kinds by providing \n",
            "relevant image descriptions. \n",
            "E-commerce: Cataloging and searching the products with product \n",
            "descriptions automation. \n",
            "Healthcare sector:  Used to support the analysis of medical imaging by \n",
            "providing descriptive outputs \n",
            "Autonomous Systems:  Improves perception and decision -making for self -\n",
            "driving cars and robotics. \n",
            "Social Media: Facebook and Instagram could apply this technology to auto -\n",
            "generate captions for all uploaded images to increase accessibility and user \n",
            "attraction. And automatic captions help people discover your content as well \n",
            "as improve its personalization. \n",
            " \n",
            "            4.3 Further Work: \n",
            " \n",
            "Further improvements may be done on: \n",
            "More Multilingual Generating: Scaling up the model to output captions in \n",
            "various languages with increased fluency and accuracy. \n",
            "Real-time Captioning:  Transforming the work done in time -efficient \n",
            "manner so that it can be used in some real-time scenarios. \n",
            "Multi-Modal Integration: Audio-Visual streams for Multimodal Captioning \n",
            "Domain Adaptation: Finetuning the model for specific domains, including \n",
            "scientific, surveillance, or educational content.21 \n",
            "22067814 Miraj Deep Bhandari \n",
            "5) References: \n",
            " \n",
            "Computer Vision for Visual Computing: Techniques and Applications. (1998). Computer \n",
            "Vision and Image Understanding, 71(2), p.153. \n",
            "doi:https://doi.org/10.1006/cviu.1998.0714. \n",
            " \n",
            "Rao, D. and McMahan, B. (2019). Natural Language Processing with PyTorch. ‘O’Reilly \n",
            "Media, Inc.’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Splittting the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)\n",
        "chunks = text_splitter.create_documents([allwords])"
      ],
      "metadata": {
        "id": "u_j34Grm17Hz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_prompt=\"\"\"\n",
        "Please summarize the below Document. The summary must be very clear with proper phrases and easy words:\n",
        "Document:`{text}'\n",
        "Summary:\n",
        "\"\"\"\n",
        "map_prompt_template=PromptTemplate(input_variables=['text'], template=chunks_prompt)\n"
      ],
      "metadata": {
        "id": "eaK1VhVz2AF9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"\"\"\n",
        "You are tasked with generating a minimum of 20 multiple-choice questions (MCQs) based on the provided text. Each MCQ should be well-structured and follow the format below:\n",
        "\n",
        "Question: [Clearly state the question in a concise and understandable manner.]\n",
        "Options:\n",
        "A) [Option 1]\n",
        "B) [Option 2]\n",
        "C) [Option 3]\n",
        "D) [Option 4]\n",
        "Correct Answer: [Specify the correct option]\n",
        "\n",
        "Ensure that questions assess key concepts, facts, or inferences from the text. Options should be plausible and not overly obvious. Avoid ambiguous wording and ensure that each question has only one correct answer. Provide a mix of factual, conceptual, and application-based questions. Each question should be self-contained and understandable without additional context.\n",
        "\n",
        "i wnat like this\n",
        "\n",
        "**Question X:**\n",
        "[Insert question here]\n",
        "\n",
        "Options:\n",
        "A) [Option A]\n",
        "B) [Option B]\n",
        "C) [Option C]\n",
        "D) [Option D]\n",
        "\n",
        "Correct Answer: [Correct option letter]\n",
        "\n",
        "\n",
        "Note: Do not include any preamble. \"\"\"\n",
        "\n",
        "human=\"{text}\" # yesma sab chunk ko summary combine vayera input hunxa     # yo ra mathi ko chunk prompt ma same hunparxa input field ko name\n",
        "\n",
        "\n",
        "final_combine_prompt=ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(system),\n",
        "                                                 HumanMessagePromptTemplate.from_template(human)])\n"
      ],
      "metadata": {
        "id": "0kF83m5z2Aui"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=\"\")"
      ],
      "metadata": {
        "id": "-TQGL6CC2NC2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summary_chain = load_summarize_chain( llm=llm_model, chain_type='map_reduce', map_prompt=map_prompt_template, combine_prompt=final_combine_prompt,\n",
        "                                         verbose=False)\n",
        "\n",
        "output = summary_chain.invoke(chunks)\n"
      ],
      "metadata": {
        "id": "b47xK7ka2Nmn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['output_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RmmUeAT2P3B",
        "outputId": "bb93e6a4-da93-4af1-cf11-7da7837d243c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Question 1:**\n",
            "What is the primary goal of the image captioning system?\n",
            "Options:\n",
            "A) To generate accurate captions for images in Nepali\n",
            "B) To improve the efficiency of image retrieval\n",
            "C) To assist self-driving cars in navigation\n",
            "D) To replace human image describers\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 2:**\n",
            "Which technique is used for image analysis in the system?\n",
            "Options:\n",
            "A) Recurrent Neural Networks\n",
            "B) Vision Transformer\n",
            "C) Convolutional Neural Networks\n",
            "D) Generative Adversarial Networks\n",
            "Correct Answer: B\n",
            "\n",
            "**Question 3:**\n",
            "What is the purpose of the GPT component in the system?\n",
            "Options:\n",
            "A) To extract features from the image\n",
            "B) To generate captions based on the extracted features\n",
            "C) To evaluate the accuracy of the generated captions\n",
            "D) To store the image dataset\n",
            "Correct Answer: B\n",
            "\n",
            "**Question 4:**\n",
            "Which metrics are likely used to evaluate the system's performance?\n",
            "Options:\n",
            "A) Caption accuracy and relevance\n",
            "B) Image similarity score\n",
            "C) Caption length\n",
            "D) Execution time\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 5:**\n",
            "What is one potential benefit of the image captioning system?\n",
            "Options:\n",
            "A) Enhancing accessibility for visually impaired individuals\n",
            "B) Reducing the cost of image annotation\n",
            "C) Increasing the speed of image search\n",
            "D) Eliminating the need for human caption writers\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 6:**\n",
            "Which of the following is NOT a key concept in modern image captioning?\n",
            "Options:\n",
            "A) Encoder-Decoder Architecture\n",
            "B) Attention Mechanisms\n",
            "C) Transformers\n",
            "D) Image Segmentation\n",
            "Correct Answer: D\n",
            "\n",
            "**Question 7:**\n",
            "What is the purpose of attention mechanisms in image captioning?\n",
            "Options:\n",
            "A) To identify the most relevant pixels in the image\n",
            "B) To capture the sequential dependencies in the caption\n",
            "C) To focus on specific areas of the image while generating captions\n",
            "D) To improve the computational efficiency of the model\n",
            "Correct Answer: C\n",
            "\n",
            "**Question 8:**\n",
            "Which transformer architecture is used in the system?\n",
            "Options:\n",
            "A) ViT\n",
            "B) BERT\n",
            "C) GPT-3\n",
            "D) RoBERTa\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 9:**\n",
            "What are the advantages of using a transformer architecture for image captioning?\n",
            "Options:\n",
            "A) Scalability, fluency, and better handling of complex scenes\n",
            "B) Improved accuracy and detail in captions\n",
            "C) Reduced computational cost\n",
            "D) Faster training time\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 10:**\n",
            "Which of the following algorithms is used in the Vision Transformer?\n",
            "Options:\n",
            "A) Positional Encoding\n",
            "B) Masked Attention\n",
            "C) Self-Attention\n",
            "D) All of the above\n",
            "Correct Answer: D\n",
            "\n",
            "**Question 11:**\n",
            "What is the role of the pre-trained tokenizer in the image captioning system?\n",
            "Options:\n",
            "A) To generate new tokens for the Nepali language\n",
            "B) To help the GPT model generate fluent and grammatically correct captions\n",
            "C) To reduce the computational cost of caption generation\n",
            "D) To improve the accuracy of the generated captions\n",
            "Correct Answer: B\n",
            "\n",
            "**Question 12:**\n",
            "What is a potential application of the image captioning system in the medical field?\n",
            "Options:\n",
            "A) Assisting in medical image analysis\n",
            "B) Generating automated medical reports\n",
            "C) Replacing human radiologists\n",
            "D) Storing and managing patient images\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 13:**\n",
            "Which of the following is NOT a technique used in the image captioning system?\n",
            "Options:\n",
            "A) Concatenated Outputs\n",
            "B) Masked Attention\n",
            "C) Image Segmentation\n",
            "D) Normalization\n",
            "Correct Answer: C\n",
            "\n",
            "**Question 14:**\n",
            "What is the purpose of the Fully Connected Network (FCN) in the model?\n",
            "Options:\n",
            "A) To connect all neurons in one layer to all neurons in the next\n",
            "B) To generate the final caption\n",
            "C) To extract features from the image\n",
            "D) To normalize the inputs\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 15:**\n",
            "What is the significance of cross-attention in image captioning?\n",
            "Options:\n",
            "A) It allows the model to attend to different sequences\n",
            "B) It improves the accuracy of the generated captions\n",
            "C) It reduces the computational cost of the model\n",
            "D) It eliminates the need for pre-training\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 16:**\n",
            "Which of the following is a limitation of the image captioning system?\n",
            "Options:\n",
            "A) It may not accurately capture complex or ambiguous images\n",
            "B) It requires extensive training data\n",
            "C) It is not real-time\n",
            "D) It is not adaptable to different languages\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 17:**\n",
            "What is a potential future improvement for the image captioning system?\n",
            "Options:\n",
            "A) Generating captions in multiple languages\n",
            "B) Making the model work in real-time\n",
            "C) Adding audio and video to the model\n",
            "D) All of the above\n",
            "Correct Answer: D\n",
            "\n",
            "**Question 18:**\n",
            "How does the image captioning system help people with visual impairments?\n",
            "Options:\n",
            "A) By providing audio descriptions of images\n",
            "B) By generating text descriptions that can be read aloud\n",
            "C) By enhancing the contrast and brightness of images\n",
            "D) By converting images into tactile representations\n",
            "Correct Answer: B\n",
            "\n",
            "**Question 19:**\n",
            "What is the purpose of using a \"pseudocode\" in the system's design?\n",
            "Options:\n",
            "A) To provide a high-level overview of the algorithm\n",
            "B) To generate executable code directly\n",
            "C) To replace the need for detailed documentation\n",
            "D) To improve the readability of the code\n",
            "Correct Answer: A\n",
            "\n",
            "**Question 20:**\n",
            "Which of the following is NOT a component of the \"ImageCaptioningSystem\" class?\n",
            "Options:\n",
            "A) preprocessImage\n",
            "B) generateCaption\n",
            "C) trainModel\n",
            "D) evaluateModel\n",
            "Correct Answer: D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = output['output_text']\n",
        "\n",
        "def parse_questions(text):\n",
        "    questions = {}\n",
        "    question_blocks = re.split(r'\\*\\*Question (\\d+):\\*\\*', text)[1:]  # Splitting on **Question X:** pattern\n",
        "\n",
        "    for i in range(0, len(question_blocks), 2):\n",
        "        q_number = int(question_blocks[i].strip())  # Extract question number\n",
        "        q_content = question_blocks[i + 1].strip().split(\"\\n\")  # Extract question content\n",
        "\n",
        "        question_text = q_content[0].strip()\n",
        "        options = {}\n",
        "        correct_answer = \"\"\n",
        "\n",
        "        for line in q_content[1:]:\n",
        "            match = re.match(r\"([A-D])\\) (.+)\", line.strip())  # Match option pattern\n",
        "            if match:\n",
        "                options[match[1]] = match[2]\n",
        "            elif line.startswith(\"Correct Answer:\"):\n",
        "                correct_answer = line.split(\":\")[1].strip()\n",
        "\n",
        "        questions[q_number] = {\n",
        "            \"question\": question_text,\n",
        "            \"options\": options,\n",
        "            \"correct_answer\": correct_answer\n",
        "        }\n",
        "\n",
        "    return questions\n",
        "\n",
        "question_dict = parse_questions(text)\n",
        "print(question_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtMnRCKlNo98",
        "outputId": "f1993abf-7418-49a4-f964-8c2ee6308f51"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: {'question': 'What is the primary goal of the image captioning system?', 'options': {'A': 'To generate accurate captions for images in Nepali', 'B': 'To improve the efficiency of image retrieval', 'C': 'To assist self-driving cars in navigation', 'D': 'To replace human image describers'}, 'correct_answer': 'A'}, 2: {'question': 'Which technique is used for image analysis in the system?', 'options': {'A': 'Recurrent Neural Networks', 'B': 'Vision Transformer', 'C': 'Convolutional Neural Networks', 'D': 'Generative Adversarial Networks'}, 'correct_answer': 'B'}, 3: {'question': 'What is the purpose of the GPT component in the system?', 'options': {'A': 'To extract features from the image', 'B': 'To generate captions based on the extracted features', 'C': 'To evaluate the accuracy of the generated captions', 'D': 'To store the image dataset'}, 'correct_answer': 'B'}, 4: {'question': \"Which metrics are likely used to evaluate the system's performance?\", 'options': {'A': 'Caption accuracy and relevance', 'B': 'Image similarity score', 'C': 'Caption length', 'D': 'Execution time'}, 'correct_answer': 'A'}, 5: {'question': 'What is one potential benefit of the image captioning system?', 'options': {'A': 'Enhancing accessibility for visually impaired individuals', 'B': 'Reducing the cost of image annotation', 'C': 'Increasing the speed of image search', 'D': 'Eliminating the need for human caption writers'}, 'correct_answer': 'A'}, 6: {'question': 'Which of the following is NOT a key concept in modern image captioning?', 'options': {'A': 'Encoder-Decoder Architecture', 'B': 'Attention Mechanisms', 'C': 'Transformers', 'D': 'Image Segmentation'}, 'correct_answer': 'D'}, 7: {'question': 'What is the purpose of attention mechanisms in image captioning?', 'options': {'A': 'To identify the most relevant pixels in the image', 'B': 'To capture the sequential dependencies in the caption', 'C': 'To focus on specific areas of the image while generating captions', 'D': 'To improve the computational efficiency of the model'}, 'correct_answer': 'C'}, 8: {'question': 'Which transformer architecture is used in the system?', 'options': {'A': 'ViT', 'B': 'BERT', 'C': 'GPT-3', 'D': 'RoBERTa'}, 'correct_answer': 'A'}, 9: {'question': 'What are the advantages of using a transformer architecture for image captioning?', 'options': {'A': 'Scalability, fluency, and better handling of complex scenes', 'B': 'Improved accuracy and detail in captions', 'C': 'Reduced computational cost', 'D': 'Faster training time'}, 'correct_answer': 'A'}, 10: {'question': 'Which of the following algorithms is used in the Vision Transformer?', 'options': {'A': 'Positional Encoding', 'B': 'Masked Attention', 'C': 'Self-Attention', 'D': 'All of the above'}, 'correct_answer': 'D'}, 11: {'question': 'What is the role of the pre-trained tokenizer in the image captioning system?', 'options': {'A': 'To generate new tokens for the Nepali language', 'B': 'To help the GPT model generate fluent and grammatically correct captions', 'C': 'To reduce the computational cost of caption generation', 'D': 'To improve the accuracy of the generated captions'}, 'correct_answer': 'B'}, 12: {'question': 'What is a potential application of the image captioning system in the medical field?', 'options': {'A': 'Assisting in medical image analysis', 'B': 'Generating automated medical reports', 'C': 'Replacing human radiologists', 'D': 'Storing and managing patient images'}, 'correct_answer': 'A'}, 13: {'question': 'Which of the following is NOT a technique used in the image captioning system?', 'options': {'A': 'Concatenated Outputs', 'B': 'Masked Attention', 'C': 'Image Segmentation', 'D': 'Normalization'}, 'correct_answer': 'C'}, 14: {'question': 'What is the purpose of the Fully Connected Network (FCN) in the model?', 'options': {'A': 'To connect all neurons in one layer to all neurons in the next', 'B': 'To generate the final caption', 'C': 'To extract features from the image', 'D': 'To normalize the inputs'}, 'correct_answer': 'A'}, 15: {'question': 'What is the significance of cross-attention in image captioning?', 'options': {'A': 'It allows the model to attend to different sequences', 'B': 'It improves the accuracy of the generated captions', 'C': 'It reduces the computational cost of the model', 'D': 'It eliminates the need for pre-training'}, 'correct_answer': 'A'}, 16: {'question': 'Which of the following is a limitation of the image captioning system?', 'options': {'A': 'It may not accurately capture complex or ambiguous images', 'B': 'It requires extensive training data', 'C': 'It is not real-time', 'D': 'It is not adaptable to different languages'}, 'correct_answer': 'A'}, 17: {'question': 'What is a potential future improvement for the image captioning system?', 'options': {'A': 'Generating captions in multiple languages', 'B': 'Making the model work in real-time', 'C': 'Adding audio and video to the model', 'D': 'All of the above'}, 'correct_answer': 'D'}, 18: {'question': 'How does the image captioning system help people with visual impairments?', 'options': {'A': 'By providing audio descriptions of images', 'B': 'By generating text descriptions that can be read aloud', 'C': 'By enhancing the contrast and brightness of images', 'D': 'By converting images into tactile representations'}, 'correct_answer': 'B'}, 19: {'question': 'What is the purpose of using a \"pseudocode\" in the system\\'s design?', 'options': {'A': 'To provide a high-level overview of the algorithm', 'B': 'To generate executable code directly', 'C': 'To replace the need for detailed documentation', 'D': 'To improve the readability of the code'}, 'correct_answer': 'A'}, 20: {'question': 'Which of the following is NOT a component of the \"ImageCaptioningSystem\" class?', 'options': {'A': 'preprocessImage', 'B': 'generateCaption', 'C': 'trainModel', 'D': 'evaluateModel'}, 'correct_answer': 'D'}}\n"
          ]
        }
      ]
    }
  ]
}